var documenterSearchIndex = {"docs":
[{"location":"api/","page":"api","title":"api","text":"Modules = [DifferentialEvolutionMCMC]\nOrder   = [:type, :function]\nPrivate = false","category":"page"},{"location":"api/#DifferentialEvolutionMCMC.DE","page":"api","title":"DifferentialEvolutionMCMC.DE","text":"function DE(;\n    n_groups = 4, \n    priors = nothing, \n    Np, \n    burnin = 1000, \n    discard_burnin = true, \n    α = .1,\n    β = .1, \n    ϵ = .001,\n    σ = .05, \n    κ = 1.0, \n    θsnooker = 0.0, \n    bounds, \n    n_initial = 0, \n    generate_proposal = random_gamma, \n    update_particle! = mh_update!,\n    evaluate_fitness! = compute_posterior!, \n    sample = sample,\n    blocking_on = x -> false,\n    blocks = [false]\n)\n\nDifferential Evolution MCMC object.\n\nKeywords\n\nn_groups=4: number of groups of particles. \nNp: number of particles per group.\nburnin=1000: number of burnin iterations\ndiscard_burnin: indicates whether burnin samples are discarded. Default is true.\nα=.1: migration probability.\nβ=.1: mutation probability.\nϵ=.001: noise in crossover step.\nσ=.05: standard deviation of noise added to parameters for mutation.\nκ=1.0: recombination with probability (1-κ) during crossover.\nθsnooker=0: sample along line x_i - z. 0.1 is recommended if > 0.\nn_initial: initial number of samples from the prior distribution when sample=resample. 10 times the number of parameters\n\nis a typical value\n\nbounds: a vector of tuples for lower and upper bounds of each parameter\niter: current iteration\ngenerate_proposal: a function that generates proposals. Default is the two mode proposal described in\n\nTurner et al. 2012. You can also choose fixed_gamma, variable_gamma (see help) or pass a custom function\n\nupdate_particle!: a function for updating the particle with a proposal value. Default: mh_update!, which uses the \n\nMetropolis-Hastings rule.\n\nevaluate_fitness!: a function for evaluating the fitness of a posterior. The default is to compute the posterior loglikelihood with \n\ncompute_posterior!. Select evaluate_fun! for optimization rather than MCMC sampling.\n\nsample: a function for sampling particles during the crossover step. The default sample uses current particle\n\nparameter values whereas resample samples from the history of accepted values for each particle. Np must 3 or greater  when using resample.\n\nblocking_on = x -> false: a function that indicates whether block updating is used on each iteration. The function requires optimization_tests\n\narguement for the DE sampler object and must return a true or false value. \n\nblocks: a vector of boolean vectors indicating which parameters to update. Each sub-vector represents a \n\nblock and each element in the sub-vector indicates which parameters are updated within the block. For example, [[true,false],[false,true]] indicates that the parameter in the first position is updated on the first block and the parameter in the second position is updated on the  second block. If a parameter is a vector or matrix, they are nested within the block sub-vector. \n\nReferences\n\nTer Braak, C. J. A Markov Chain Monte Carlo version of the genetic algorithm Differential Evolution: easy Bayesian computing for real parameter spaces.\nTer Braak, Cajo JF, and Jasper A. Vrugt. \"Differential evolution Markov chain with snooker updater and fewer chains.\" Statistics and Computing 18.4 (2008): 435-446\nTurner, B. M., Sederberg, P. B., Brown, S. D., & Steyvers, M. (2013). A method for efficiently sampling from distributions with correlated dimensions. Psychological methods, 18(3), 368.\nTurner, B. M., & Sederberg, P. B. (2012). Approximate Bayesian computation with differential evolution. Journal of Mathematical Psychology, 56(5), 375-385.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentialEvolutionMCMC.DEModel","page":"api","title":"DifferentialEvolutionMCMC.DEModel","text":"function DEModel(\n    args...; \n    prior_loglike = nothing, \n    loglike, \n    names, \n    sample_prior, \n    data, \n    kwargs...\n)\n\nA model object containing the log likelihood function and prior distributions.\n\nKeywords\n\nargs...: optional positional arguments for loglike\nprior_loglike=nothing: log likelihood of posterior sample. A function must be \n\ndefine for sample, but not for optimize.\n\nloglike: a log likelihood function for Bayesian parameter estimation or an objective function for \n\noptimization. \n\nsample_prior: a function for initial values. Typically, a prior distribution is ideal.\nnames: parameter names\nkwargs...: optional keyword arguments for loglike\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentialEvolutionMCMC.Particle","page":"api","title":"DifferentialEvolutionMCMC.Particle","text":"Particle{T}\n\nFields\n\nθ: a vector of parameters\nsamples: a 2-dimensional array containing all acccepted proposals\naccept: proposal acceptance. 1: accept, 0: reject\nweight: particle weight based on model fit (currently posterior log likelihood)\nlp: a vector of log posterior probabilities associated with each accepted proposal\nid: particle id\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentialEvolutionMCMC.compute_posterior!-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.compute_posterior!","text":"compute_posterior!(de, model, proposal)\n\nComputes posterior log likelihood of proposal particle. The value -Inf  is returned if the proposal is out of bounds. \n\nArguments\n\nde: differential evolution object\nmodel: model containing a likelihood function with data and priors\nproposal: the proposed particle\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.evaluate_fun!-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.evaluate_fun!","text":"evaluate_fun!(de, model, proposal))\n\nEvaluates the fitness of an arbitrary function called loglike. This is used for  point estimation as it does not use a prior distribution.  \n\nArguments\n\nde: differential evolution object\nmodel: model containing a likelihood function with data and priors\nproposal: the proposed particle\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.fixed_gamma-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.fixed_gamma","text":"fixed_gamma(de, Pt, group)\n\nGenerate proposal according to θ' = θt + γ(θm − θn) + b where γ = 2.38.\n\nArguments\n\nde: differential evolution object\nPt: current particle\ngroup: a group of particles\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.optimize-Tuple{DEModel, DE, Int64}","page":"api","title":"DifferentialEvolutionMCMC.optimize","text":"optimize(model::DEModel, de::DE, n_iter::Int; progress=false, kwargs...)\n\nFinds optimal set of parameters.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of algorithm\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.optimize-Tuple{DEModel, DE, MCMCThreads, Int64}","page":"api","title":"DifferentialEvolutionMCMC.optimize","text":"optimize(model::DEModel, de::DE, ::MCMCThreads, n_iter::Int; progress=false, kwargs...)\n\nFinds optimal set of parameters.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nMCMCThreads: pass MCMCThreads() object to run on multiple threads\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of algorithm\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.random_gamma-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.random_gamma","text":"random_gamma(de, Pt, group)\n\nGenerate proposal according to θ' = θt + γ1(θm − θn) + γ2(θb − θt) + b. γ2=0 after burnin\n\nArguments\n\nde: differential evolution object\nPt: current particle\ngroup: a group of particles\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.resample-NTuple{4, Any}","page":"api","title":"DifferentialEvolutionMCMC.resample","text":"resample(de, group, n, replace)\n\nSample a random particle from previously accepted values for snooker update.\n\nArguments\n\nde: differential evolution object\ngroup: a group of particles\nn: number of particles to sample\nreplace: sample with replacement if true\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.variable_gamma-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.variable_gamma","text":"variable_gamma(de, Pt, group)\n\nGenerate proposal according to θ' = θt + γ(θm − θn) + b where γ = 2.38/√(2d) where d is the number of parameters\n\nArguments\n\nde: differential evolution object\nPt: current particle\ngroup: a group of particles\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{DE, Any, Any, Any}","page":"api","title":"StatsBase.sample","text":"sample(de::DE, group_diff, n, replace)\n\nSample a random particle.\n\nArguments\n\nde: differential evolution object\ngroup: a group of particles\nn: number of particles to sample\nreplace: sample with replacement if true\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{DEModel, DE, Int64}","page":"api","title":"StatsBase.sample","text":"sample(model::DEModel, de::DE, n_iter::Int; progress=false, kwargs...)\n\nSamples from the posterior distribution.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of sampler\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{DEModel, DE, MCMCThreads, Int64}","page":"api","title":"StatsBase.sample","text":"sample(model::DEModel, de::DE, ::MCMCThreads, n_iter::Int; progress=false, kwargs...)\n\nSamples from the posterior distribution with each group of particles on a seperarate thread for the mutation and crossover steps.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nMCMCThreads: pass MCMCThreads() object to run on multiple threads\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of sampler\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"using DifferentialEvolutionMCMC\nusing DifferentialEvolutionMCMC: minimize!\nusing Random \nRandom.seed!(6845)\n\nfunction sample_prior()\n    return [rand(Uniform(-5, 5), 2)]\n end\n\n function rastrigin(_, x)\n     A = 10.0\n     n = length(x)\n     y = A * n\n     for  i in 1:n\n         y +=  + x[i]^2 - A * cos(2 * π * x[i])\n     end\n     return y \n end\n\nbounds = ((-5.0,5.0),)\nnames = (:x,)\n\nmodel = DEModel(; \n    sample_prior, \n    loglike = rastrigin, \n    data = nothing,\n    names)\n\nde = DE(;\n    sample_prior,\n    bounds,\n    Np = 10, \n    n_groups = 1, \n    update_particle! = minimize!,\n    evaluate_fitness! = evaluate_fun!)","category":"page"},{"location":"optimization/#Optimization-Example","page":"Optimization","title":"Optimization Example","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"The purpose of this example is to demonstrate how to optimize a function as opposed to perform Bayesian parameter estimation.","category":"page"},{"location":"optimization/#Load-Packages","page":"Optimization","title":"Load Packages","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"Our first step is to load the required packages.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"using DifferentialEvolutionMCMC\nusing DifferentialEvolutionMCMC: minimize!\nusing Distributions\nusing Random \nRandom.seed!(6845)","category":"page"},{"location":"optimization/#Initialize-the-DE-Algorithm","page":"Optimization","title":"Initialize the DE Algorithm","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"Each particle in DifferentialEvolution must be seeded with an initial value. To do so, we define a function that returns the initial value. Even though we are not performing Bayesian parameter estimation, we want to use prior information to intelligently seed the DE algorithm. In our case, we will return a vector contained in a vector. ","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"function sample_prior()\n    return [rand(Uniform(-5, 5), 2)]\n end","category":"page"},{"location":"optimization/#Objective-Function","page":"Optimization","title":"Objective Function","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"In this example, we will find the minimum of the rastrigin, which challenging due to its \"egg carton-like\" surface containing several local minima. ","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"(Image: )","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"The minimum of the rastrigin function is the zero vector for input x. The code block below defines the rastrigin function for an input vector of an arbitrary length. Since we will not be passing data to the function, we will set the first argument to _ and assign it a value of nothing below.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"function rastrigin(_, x)\n    A = 10.0\n    n = length(x)\n    y = A * n\n    for  i in 1:n\n        y +=  + x[i]^2 - A * cos(2 * π * x[i])\n    end\n    return y \nend","category":"page"},{"location":"optimization/#Define-Bounds-and-Parameter-Names","page":"Optimization","title":"Define Bounds and Parameter Names","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"We must define the lower and upper bounds of each parameter. The bounds are a Tuple of tuples, where the i^mathrmth inner tuple contains the lower and upper bounds of the i^mathrmth parameter. In the rastrigin function, x is a vector with an unspecified length. The bounds below applies to each element in x.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"bounds = ((-5.0,5.0),)","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"We can also pass a Tuple of names for the argument x.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"names = (:x,)","category":"page"},{"location":"optimization/#Define-Model-Object","page":"Optimization","title":"Define Model Object","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"Now that we have defined the necessary components of our model, we will organize them into a DEModel object as shown below. Notice that data = nothing because we do not need data for this optimization problem.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"model = DEModel(; \n    sample_prior, \n    loglike = rastrigin, \n    data = nothing,\n    names)","category":"page"},{"location":"optimization/#Define-Sampler","page":"Optimization","title":"Define Sampler","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"Next, we will create a sampler with the constructor DE. Here, we will pass the sample_prior function and the variable bounds, which constains the lower and upper bounds of each parameter. In addition, we will specify 1000 burnin samples, Np=10 particles for n_groups=1 groups. In the last two keyword arguments, we use minimize! for the update_particle! function because we want to minimize rastrigin, and evaluate_fitness! = evaluate_fun! because we do not want to incorporate the prior log likelihood into our evaluation of rastrigin.","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"de = DE(;\n    sample_prior,\n    bounds,\n    Np = 10, \n    n_groups = 1, \n    update_particle! = minimize!,\n    evaluate_fitness! = evaluate_fun!)","category":"page"},{"location":"optimization/#Optimize-the-Function","page":"Optimization","title":"Optimize the Function","text":"","category":"section"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"The code block below runs the optimizer for 2000 iterations with the progress bar. ","category":"page"},{"location":"optimization/","page":"Optimization","title":"Optimization","text":"n_iter = 2000\nparticles = optimize(model, de, n_iter, progress=true);\nresults = get_optimal(de, model, particles)","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(6541)\n\ndata = rand(Normal(0.0, 1.0), 50)\n\nfunction prior_loglike(μ, σ)\n    LL = 0.0\n    LL += logpdf(Normal(0, 1), μ)\n    LL += logpdf(truncated(Cauchy(0, 1), 0, Inf), σ)\n    return LL\nend\n\nfunction sample_prior()\n    μ = rand(Normal(0, 1))\n    σ = rand(truncated(Cauchy(0, 1), 0, Inf))\n    return [μ,σ]\nend\n\nnames = (:μ,:σ)\nbounds = ((-Inf,Inf),(0.0,Inf))\n\nfunction loglike(data, μ, σ)\n    return sum(logpdf.(Normal(μ, σ), data))\nend\n\nmodel = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names\n)\n\nde = DE(;sample_prior, bounds, burnin = 1000, Np = 6)\n\nn_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"gaussian/#Gaussian-Example","page":"Gaussian Model","title":"Gaussian Example","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"This simple example using a Gaussian model will guide you through the process of performing Bayesin parameter estimation with Differential Evolution MCMC. Suppose we observe mathbfy = lefty_1y_2 dots y_50 right which are assumed to follow a Gaussian Distribution. Thus, we can write the following sampling statement:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"mathbfy sim mathrmnormal(mu sigma)","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"where theta is a parameter representing the probability of success. Our goal is to estimate the probability of success theta from the k. Let's assume the prior distribution of theta is given by","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"mu sim mathrmnormal(0 1)","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"In this simple case, the posterior distribution of theta has a simple closed-form solution:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"sigma sim mathrmCauchy(0 1)_0^infty","category":"page"},{"location":"gaussian/#Load-Packages","page":"Gaussian Model","title":"Load Packages","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Our first step is to load the required packages including StatsPlots.jl for plotting the MCMC chain.","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(6541)","category":"page"},{"location":"gaussian/#Generate-Data","page":"Gaussian Model","title":"Generate Data","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Let's assume we make N=50 using mu = 0 and sigma=1: ","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"data = rand(Normal(0.0, 1.0), 50)","category":"page"},{"location":"gaussian/#Define-Prior-Log-Likelihood-Function","page":"Gaussian Model","title":"Define Prior Log Likelihood Function","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"The next step is to define a function that passes our parameters mu and sigma and evaluates the log likelihood. The function to compute the prior log likelihood is as follows:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"function prior_loglike(μ, σ)\n    LL = 0.0\n    LL += logpdf(Normal(0, 1), μ)\n    LL += logpdf(truncated(Cauchy(0, 1), 0, Inf), σ)\n    return LL\nend","category":"page"},{"location":"gaussian/#Define-Sample-Prior-Distribution","page":"Gaussian Model","title":"Define Sample Prior Distribution","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Each particle in DifferentialEvolution must be seeded with an initial value. To do so, we define a function that returns the initial value. A common approach is to use the prior distribution as it is intended to encode likely values of mu and sigma. The function for sampling from the prior distribution of mu and sigma is given in the following code block:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"function sample_prior()\n    μ = rand(Normal(0, 1))\n    σ = rand(truncated(Cauchy(0, 1), 0, Inf))\n    return [μ,σ]\nend","category":"page"},{"location":"gaussian/#Define-Log-Likelihood-Function","page":"Gaussian Model","title":"Define Log Likelihood Function","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Next, we define a function to compute the log likelihood of the data. The first argument must be the data followed by a separate argument for each parameter, maintaining the same order specified in prior_loglike. In our case, we can write the log likelihood function with data followed by mu and sigma:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"function loglike(data, μ, σ)\n    return sum(logpdf.(Normal(μ, σ), data))\nend","category":"page"},{"location":"gaussian/#Define-Bounds-and-Parameter-Names","page":"Gaussian Model","title":"Define Bounds and Parameter Names","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"We must define the lower and upper bounds of each parameter. The bounds are a Tuple of tuples, where the i^mathrmth inner tuple contains the lower and upper bounds of the i^mathrmth parameter. Note that the parameters must be in the same order as specified in prior_loglike and loglike. In the present case, we only have mu followed by sigma:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"bounds = ((-Inf,Inf),(0.0,Inf))","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"We can also pass a Tuple of names for each parameter, again in the same order as specified in prior_loglike and loglike:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"names = (:μ,:σ)","category":"page"},{"location":"gaussian/#Define-Model-Object","page":"Gaussian Model","title":"Define Model Object","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Now that we have defined the necessary components of our model, we will organize them into a DEModel object as follows:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"model = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names)","category":"page"},{"location":"gaussian/#Define-Sampler","page":"Gaussian Model","title":"Define Sampler","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Next, we will create a sampler with the constructor DE. Here, we will pass the sample_prior function and the variable bounds, which constains the lower and upper bounds of each parameter. In addition, we will specify 1000 burnin samples, Np=6 particles per group (default 4 particles per group).","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"de = DE(;sample_prior, bounds, burnin=1000, Np=6)","category":"page"},{"location":"gaussian/#Estimate-Parameter","page":"Gaussian Model","title":"Estimate Parameter","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"The code block below runs the sampler for 2000 iterations with each group of particles running on a separate thread. The progress bar is also set to display. ","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"n_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"gaussian/#Convergence","page":"Gaussian Model","title":"Convergence","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"We will evaluate convergence using two methods. First, we will verify that hatr leq 105 in the chain summary above. Indeed, it is. Its also important to examine the trace plot for any evidence of non-stationarity, or getting stuck at the same value for long periods of time. In the left panel of the plot below, the samples for each chain display the characteristic behavior indicative of efficient sampling and convergence: they look like white noise, or a \"hairy caterpillar\". The plot on the right shows the posterior distribution of mu and sigma for each of the 12 chains.","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"plot(chains, grid=false)","category":"page"},{"location":"#DifferentialEvolutionMCMC.jl","page":"home","title":"DifferentialEvolutionMCMC.jl","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"using DifferentialEvolutionMCMC\nusing DifferentialEvolutionMCMC: sample_init\nusing DifferentialEvolutionMCMC: crossover!\nusing Distributions\nusing StatsPlots\n#using PyPlot\nusing Random\n\n#pyplot()\n\nRandom.seed!(81872)\n\ndata = rand(Normal(0.0, 1.0), 5)\n\nfunction prior_loglike(μ, σ)\n    LL = 0.0\n    LL += logpdf(Normal(0, 1), μ)\n    LL += logpdf(truncated(Cauchy(0, 1), 0, Inf), σ)\n    return LL\nend\n\nfunction sample_prior()\n    μ = rand(Normal(0, 1))\n    σ = rand(truncated(Cauchy(0, 1), 0, Inf))\n    return [μ,σ]\nend\n\nnames = (:μ,:σ)\nbounds = ((-Inf,Inf),(0.0,Inf))\n\nfunction loglike(data, μ, σ)\n    return sum(logpdf.(Normal(μ, σ), data))\nend\n\nmodel = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names)\n\nde = DE(;sample_prior, bounds, burnin = 10, n_groups = 4, Np = 8)\nn_iter = 60\n\ncolors = [RGB(.251, .388, .847),\n            RGB(.220, .596, .149),\n            RGB(.584, .345, .698),\n            RGB(.796, .235, .200)]\n\ngroups = sample_init(model, de, n_iter)\nanimation = @animate for i in 1:n_iter\n    de_plot = scatter()\n    for (j,g) in enumerate(groups)\n        crossover!(model, de, g)\n        Θ = mapreduce(p -> p.Θ, hcat, g)'\n        scatter!(de_plot, Θ[:,1], Θ[:,2], xlabel=\"μ\", ylabel = \"σ\", grid=false, leg=false,\n            xlims=(-2,2), ylims=(0,2), framestyle=:box, title = \"iteration $i\",\n            markersize=5, xaxis=font(12), yaxis=font(12), markerstrokewidth=3, color = colors[j])\n        vline!([mean(data)], color = :black, linestyle=:dash)\n        hline!([std(data)], color = :black, linestyle=:dash)\n     end\nend\ngif(animation, \"de_animation.gif\", fps = 4)","category":"page"},{"location":"","page":"home","title":"home","text":"Welcome to DifferentialEvolutionMCMC.jl. With this package, you can perform Bayesian parameter estimation using Differential Evolution MCMC (DEMCMC), and perform optimization using the basic DE algorithm  Please see the navigation panel on the left for information pertaining to the API and runnable examples. ","category":"page"},{"location":"#How-Does-it-Work?","page":"home","title":"How Does it Work?","text":"","category":"section"},{"location":"#Intuition","page":"home","title":"Intuition","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"The basic idea behind DEMCMC is that a group of interacting particles traverse the parameter space and share information about the joint posterior distribution of model parameters. Across many iterations, the samples obtained from the particles will approximate the posterior distribution. The image below illustrates how the particles sample from the posterior distribution of mu and sigma of a simple Gaussian model. In this example, five observations were sampled from a Gaussian distribution with mu=0 and sigma=10. The DEMCMC sampler consists of four color coded groups of particles which operate semi-independently from each other. Note that the dashed lines represent the maximum likelihood estimates. The particles cluster near the maximum likelihood estimates because the true parameters are close the center of the prior distributions. ","category":"page"},{"location":"","page":"home","title":"home","text":"(Image: )","category":"page"},{"location":"#Technical-Description","page":"home","title":"Technical Description","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"This section provides a more technical explanation of the basic algorithm. Please see the references below for more details. More formally, a particle p in 12dots P is a vector of n parameters in a mathbbR^n parameter space defined as:","category":"page"},{"location":"","page":"home","title":"home","text":"Theta_p = theta_p1theta_p2dots theta_pn","category":"page"},{"location":"","page":"home","title":"home","text":"On each iteration i, a new position for each particle p is proposed by adding the weighted difference of two randomly selected particles jk to particle p along with a small amount of noise. Formally, the proposal is given by:","category":"page"},{"location":"","page":"home","title":"home","text":"Theta_p^prime = Theta_p + gamma (Theta_j - Theta_k) + b","category":"page"},{"location":"","page":"home","title":"home","text":"where b sim mathrmuniform(-epsilon epsilon). DEMCMC uses the difference between randomly selected particles to leverage approximate derivatives in the proposal process. The proposal is accepted according to the Metropolis-Hastings rule whereby the proposal is always accepted if its log likelihood is greater than that of the current position, but is accepted proportionally to the ratio of log likelihoods otherwise.","category":"page"},{"location":"#References","page":"home","title":"References","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"Ter Braak, C. J. (2006). A Markov Chain Monte Carlo version of the genetic algorithm Differential Evolution: easy Bayesian computing for real parameter spaces. Statistics and Computing, 16, 239-249.","category":"page"},{"location":"","page":"home","title":"home","text":"Ter Braak, C. J., & Vrugt, J. A. (2008). Differential evolution Markov chain with snooker updater and fewer chains. Statistics and Computing, 18, 435-446.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(88484)\n\ndata = (N = 10,k = 6)\n\nprior_loglike(θ) = logpdf(Beta(1, 1), θ)\n\nsample_prior() = rand(Beta(1, 1))\n\nbounds = ((0,1),)\nnames = (:θ,)\n\nfunction loglike(data, θ)\n    (;N,k) = data\n    return logpdf(Binomial(N, θ), k)\nend\n\nmodel = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names\n)\n\nde = DE(;sample_prior, bounds, burnin=1000, Np=3, σ=.01)\nn_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"binomial/#Binomial-Example","page":"Binomial Model","title":"Binomial Example","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"This simple example using a binomial model will guide you through the process of performing Bayesin parameter estimation with Differential Evolution MCMC. Suppose we observe N samples from a random binomial process and observe k successes. Formally, this can be stated as:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"k sim mathrmBinomial(N theta)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"where theta is a parameter representing the probability of success. Our goal is to estimate the probability of success theta from the k. Let's assume the prior distribution of theta is given by:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"theta sim mathrmbeta(1 1)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"In this simple case, the posterior distribution of theta has a simple closed-form solution:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"theta_k sim mathrmbeta(1 + k 1 + N - k)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We will use this fact to verify that the MCMC sampler is working correctly. ","category":"page"},{"location":"binomial/#Load-Packages","page":"Binomial Model","title":"Load Packages","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Our first step is to load the required packages including StatsPlots.jl for plotting the MCMC chain.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(88484)","category":"page"},{"location":"binomial/#Define-Data","page":"Binomial Model","title":"Define Data","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Let's assume we make N=10 observations which produce k=6 successes. ","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"data = (N = 10,k = 6)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The true posterior distribution is given by:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"true_posterior = Beta(1 + 6, 1 + 4)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The maximum likelihood estimate of theta is","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"hattheta_mathrmMLE = frackN = 060","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The mean of the posterior of theta will be slightly less because our prior distribution encodes one failure and one success:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"mean(true_posterior)","category":"page"},{"location":"binomial/#Define-Prior-Log-Likelihood-Function","page":"Binomial Model","title":"Define Prior Log Likelihood Function","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The next step is to define a function that passes our parameter theta and evaluates the log likelihood. The function to compute the prior log likelihood is as follows:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"prior_loglike(θ) = logpdf(Beta(1, 1), θ)","category":"page"},{"location":"binomial/#Define-Sample-Prior-Distribution","page":"Binomial Model","title":"Define Sample Prior Distribution","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Each particle in DifferentialEvolution must be seeded with an initial value. To do so, we define a function that returns the initial value. A common approach is to use the prior distribution as it is intended to encode likely values of theta. The function for sampling from the prior distribution of theta is given in the following code block:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"sample_prior() = rand(Beta(1, 1))","category":"page"},{"location":"binomial/#Define-Log-Likelihood-Function","page":"Binomial Model","title":"Define Log Likelihood Function","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Next, we define a function to compute the log likelihood of the data. The first argument must be the data followed by a separate argument for each parameter, maintaining the same order specified in prior_loglike. In our case, we can write the log likelihood function as follows:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"function loglike(data, θ)\n    (;N,k) = data\n    return logpdf(Binomial(N, θ), k)\nend","category":"page"},{"location":"binomial/#Define-Bounds-and-Parameter-Names","page":"Binomial Model","title":"Define Bounds and Parameter Names","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We must define the lower and upper bounds of each parameter. The bounds are a Tuple of tuples, where the i^mathrmth inner tuple contains the lower and upper bounds of the i^mathrmth parameter. Note that the parameters must be in the same order as specified in prior_loglike and loglike. In the present case, we only have a single parameter theta:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"bounds = ((0,1),)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We can also pass a Tuple of names for each parameter, again in the same order as specified in prior_loglike and loglike:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"names = (:θ,)","category":"page"},{"location":"binomial/#Define-Model-Object","page":"Binomial Model","title":"Define Model Object","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Now that we have defined the necessary components of our model, we will organize them into a DEModel object as follows:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"model = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names)","category":"page"},{"location":"binomial/#Define-Sampler","page":"Binomial Model","title":"Define Sampler","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Next, we will create a sampler with the constructor DE. Here, we will pass the sample_prior function and the variable bounds, which constains the lower and upper bounds of each parameter. In addition, we will specify 1000 burnin samples, Np=3 particles per group (default 4 particles per group) and proposal noise of σ=.01.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"de = DE(;sample_prior, bounds, burnin=1000, Np=3, σ=.01)","category":"page"},{"location":"binomial/#Estimate-Parameter","page":"Binomial Model","title":"Estimate Parameter","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The code block below runs the sampler for 2000 iterations with each group of particles running on a separate thread. The progress bar is also set to display. ","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"n_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"binomial/#Evaluation","page":"Binomial Model","title":"Evaluation","text":"","category":"section"},{"location":"binomial/#Convergence","page":"Binomial Model","title":"Convergence","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We will evaluate convergence using two methods. First, we will verify that hatr leq 105 in the chain summary above. Indeed, it is. Its also important to examine the trace plot for any evidence of non-stationarity, or getting stuck at the same value for long periods of time. In the left panel of the plot below, the samples for each chain display the characteristic behavior indicative of efficient sampling and convergence: they look like white noise, or a \"hairy caterpillar\". The plot on the right shows the posterior distribution of theta for each of the 12 chains.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"plot(chains, grid=false)","category":"page"},{"location":"binomial/#Accuracy","page":"Binomial Model","title":"Accuracy","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Let's also see if the results are similar to the closed-form solution. Below, we see that the mean and standard deviation are similar to the chain summary in the Estimate Parameter section, suggesting that the MCMC sampler is working as expected.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"mean(true_posterior)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"std(true_posterior)","category":"page"}]
}

var documenterSearchIndex = {"docs":
[{"location":"api/","page":"api","title":"api","text":"Modules = [DifferentialEvolutionMCMC]\nOrder   = [:type, :function]\nPrivate = false","category":"page"},{"location":"api/#DifferentialEvolutionMCMC.DE","page":"api","title":"DifferentialEvolutionMCMC.DE","text":"function DE(;\n    n_groups = 4, \n    priors = nothing, \n    Np, \n    burnin = 1000, \n    discard_burnin = true, \n    α = .1,\n    β = .1, \n    ϵ = .001,\n    σ = .05, \n    κ = 1.0, \n    θsnooker = 0.0, \n    bounds, \n    n_initial = 0, \n    generate_proposal = random_gamma, \n    update_particle! = mh_update!,\n    evaluate_fitness! = compute_posterior!, \n    sample = sample,\n    blocking_on = x -> false,\n    blocks = [false]\n)\n\nDifferential Evolution MCMC object.\n\nKeywords\n\nn_groups=4: number of groups of particles. \nNp: number of particles per group.\nburnin=1000: number of burnin iterations\ndiscard_burnin: indicates whether burnin samples are discarded. Default is true.\nα=.1: migration probability.\nβ=.1: mutation probability.\nϵ=.001: noise in crossover step.\nσ=.05: standard deviation of noise added to parameters for mutation.\nκ=1.0: recombination with probability (1-κ) during crossover.\nθsnooker=0: sample along line x_i - z. 0.1 is recommended if > 0.\nn_initial: initial number of samples from the prior distribution when sample=resample. 10 times the number of parameters\n\nis a typical value\n\nbounds: a vector of tuples for lower and upper bounds of each parameter\niter: current iteration\ngenerate_proposal: a function that generates proposals. Default is the two mode proposal described in\n\nTurner et al. 2012. You can also choose fixed_gamma, variable_gamma (see help) or pass a custom function\n\nupdate_particle!: a function for updating the particle with a proposal value. Default: mh_update!, which uses the \n\nMetropolis-Hastings rule.\n\nevaluate_fitness!: a function for evaluating the fitness of a posterior. The default is to compute the posterior loglikelihood with \n\ncompute_posterior!. Select evaluate_fun! for optimization rather than MCMC sampling.\n\nsample: a function for sampling particles during the crossover step. The default sample uses current particle\n\nparameter values whereas resample samples from the history of accepted values for each particle. Np must 3 or greater  when using resample.\n\nblocking_on = x -> false: a function that indicates whether block updating is used on each iteration. The function requires optimization_tests\n\narguement for the DE sampler object and must return a true or false value. \n\nblocks: a vector of boolean vectors indicating which parameters to update. Each sub-vector represents a \n\nblock and each element in the sub-vector indicates which parameters are updated within the block. For example, [[true,false],[false,true]] indicates that the parameter in the first position is updated on the first block and the parameter in the second position is updated on the  second block. If a parameter is a vector or matrix, they are nested within the block sub-vector. \n\nReferences\n\nTer Braak, C. J. A Markov Chain Monte Carlo version of the genetic algorithm Differential Evolution: easy Bayesian computing for real parameter spaces.\nTer Braak, Cajo JF, and Jasper A. Vrugt. \"Differential evolution Markov chain with snooker updater and fewer chains.\" Statistics and Computing 18.4 (2008): 435-446\nTurner, B. M., Sederberg, P. B., Brown, S. D., & Steyvers, M. (2013). A method for efficiently sampling from distributions with correlated dimensions. Psychological methods, 18(3), 368.\nTurner, B. M., & Sederberg, P. B. (2012). Approximate Bayesian computation with differential evolution. Journal of Mathematical Psychology, 56(5), 375-385.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentialEvolutionMCMC.DEModel","page":"api","title":"DifferentialEvolutionMCMC.DEModel","text":"function DEModel(\n    args...; \n    prior_loglike = nothing, \n    loglike, \n    names, \n    sample_prior, \n    data, \n    kwargs...\n)\n\nA model object containing the log likelihood function and prior distributions.\n\nKeywords\n\nargs...: optional positional arguments for loglike\nprior_loglike=nothing: log likelihood of posterior sample. A function must be \n\ndefine for sample, but not for optimize.\n\nloglike: a log likelihood function for Bayesian parameter estimation or an objective function for \n\noptimization. \n\nsample_prior: a function for initial values. Typically, a prior distribution is ideal.\nnames: parameter names\nkwargs...: optional keyword arguments for loglike\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentialEvolutionMCMC.Particle","page":"api","title":"DifferentialEvolutionMCMC.Particle","text":"Particle{T}\n\nFields\n\nθ: a vector of parameters\nsamples: a 2-dimensional array containing all acccepted proposals\naccept: proposal acceptance. 1: accept, 0: reject\nweight: particle weight based on model fit (currently posterior log likelihood)\nlp: a vector of log posterior probabilities associated with each accepted proposal\nid: particle id\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentialEvolutionMCMC.compute_posterior!-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.compute_posterior!","text":"compute_posterior!(de, model, proposal)\n\nComputes posterior log likelihood of proposal particle. The value -Inf  is returned if the proposal is out of bounds. \n\nArguments\n\nde: differential evolution object\nmodel: model containing a likelihood function with data and priors\nproposal: the proposed particle\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.evaluate_fun!-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.evaluate_fun!","text":"evaluate_fun!(de, model, proposal))\n\nEvaluates the fitness of an arbitrary function called loglike. This is used for  point estimation as it does not use a prior distribution.  \n\nArguments\n\nde: differential evolution object\nmodel: model containing a likelihood function with data and priors\nproposal: the proposed particle\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.fixed_gamma-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.fixed_gamma","text":"fixed_gamma(de, Pt, group)\n\nGenerate proposal according to θ' = θt + γ(θm − θn) + b where γ = 2.38.\n\nArguments\n\nde: differential evolution object\nPt: current particle\ngroup: a group of particles\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.optimize-Tuple{DEModel, DE, Int64}","page":"api","title":"DifferentialEvolutionMCMC.optimize","text":"optimize(model::DEModel, de::DE, n_iter::Int; progress=false, kwargs...)\n\nFinds optimal set of parameters.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of algorithm\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.optimize-Tuple{DEModel, DE, MCMCThreads, Int64}","page":"api","title":"DifferentialEvolutionMCMC.optimize","text":"optimize(model::DEModel, de::DE, ::MCMCThreads, n_iter::Int; progress=false, kwargs...)\n\nFinds optimal set of parameters.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nMCMCThreads: pass MCMCThreads() object to run on multiple threads\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of algorithm\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.random_gamma-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.random_gamma","text":"random_gamma(de, Pt, group)\n\nGenerate proposal according to θ' = θt + γ1(θm − θn) + γ2(θb − θt) + b. γ2=0 after burnin\n\nArguments\n\nde: differential evolution object\nPt: current particle\ngroup: a group of particles\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.resample-NTuple{4, Any}","page":"api","title":"DifferentialEvolutionMCMC.resample","text":"resample(de, group, n, replace)\n\nSample a random particle from previously accepted values for snooker update.\n\nArguments\n\nde: differential evolution object\ngroup: a group of particles\nn: number of particles to sample\nreplace: sample with replacement if true\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentialEvolutionMCMC.variable_gamma-Tuple{Any, Any, Any}","page":"api","title":"DifferentialEvolutionMCMC.variable_gamma","text":"variable_gamma(de, Pt, group)\n\nGenerate proposal according to θ' = θt + γ(θm − θn) + b where γ = 2.38/√(2d) where d is the number of parameters\n\nArguments\n\nde: differential evolution object\nPt: current particle\ngroup: a group of particles\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{DE, Any, Any, Any}","page":"api","title":"StatsBase.sample","text":"sample(de::DE, group_diff, n, replace)\n\nSample a random particle.\n\nArguments\n\nde: differential evolution object\ngroup: a group of particles\nn: number of particles to sample\nreplace: sample with replacement if true\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{DEModel, DE, Int64}","page":"api","title":"StatsBase.sample","text":"sample(model::DEModel, de::DE, n_iter::Int; progress=false, kwargs...)\n\nSamples from the posterior distribution.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of sampler\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.sample-Tuple{DEModel, DE, MCMCThreads, Int64}","page":"api","title":"StatsBase.sample","text":"sample(model::DEModel, de::DE, ::MCMCThreads, n_iter::Int; progress=false, kwargs...)\n\nSamples from the posterior distribution with each group of particles on a seperarate thread for the mutation and crossover steps.\n\nArguments\n\nmodel: a model containing likelihood function with data and priors\nde: differential evolution object\nMCMCThreads: pass MCMCThreads() object to run on multiple threads\nn_iter: number of iterations or samples\n\nKeywords\n\nprogress=false: show progress of sampler\nkwargs...: optional keyword arguments\n\n\n\n\n\n","category":"method"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(6541)\n\ndata = rand(Normal(0.0, 1.0), 50)\n\nfunction prior_loglike(μ, σ)\n    LL = 0.0\n    LL += logpdf(Normal(0, 1), μ)\n    LL += logpdf(truncated(Cauchy(0, 1), 0, Inf), σ)\n    return LL\nend\n\nfunction sample_prior()\n    μ = rand(Normal(0, 1))\n    σ = rand(truncated(Cauchy(0, 1), 0, Inf))\n    return [μ,σ]\nend\n\nnames = (:μ,:σ)\nbounds = ((-Inf,Inf),(0.0,Inf))\n\nfunction loglike(data, μ, σ)\n    return sum(logpdf.(Normal(μ, σ), data))\nend\n\nmodel = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names\n)\n\nde = DE(;sample_prior, bounds, burnin = 1000, Np = 6)\n\nn_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"gaussian/#Gaussian-Example","page":"Gaussian Model","title":"Gaussian Example","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"This simple example using a Gaussian model will guide you through the process of performing Bayesin parameter estimation with Differential Evolution MCMC. Suppose we observe mathbfy = lefty_1y_2 dots y_50 right which are assumed to follow a Gaussian Distribution. Thus, we can write the following sampling statement:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"mathbfy sim mathrmnormal(mu sigma)","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"where theta is a parameter representing the probability of success. Our goal is to estimate the probability of success theta from the k. Let's assume the prior distribution of theta is given by","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"mu sim mathrmnormal(0 1)","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"In this simple case, the posterior distribution of theta has a simple closed-form solution:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"sigma sim mathrmCauchy(0 1)_0^infty","category":"page"},{"location":"gaussian/#Load-Packages","page":"Gaussian Model","title":"Load Packages","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Our first step is to load the required packages including StatsPlots.jl for plotting the MCMC chain.","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(6541)","category":"page"},{"location":"gaussian/#Generate-Data","page":"Gaussian Model","title":"Generate Data","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Let's assume we make N=50 using mu = 0 and sigma=1: ","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"data = rand(Normal(0.0, 1.0), 50)","category":"page"},{"location":"gaussian/#Define-Prior-Log-Likelihood-Function","page":"Gaussian Model","title":"Define Prior Log Likelihood Function","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"The next step is to define a function that passes our parameters mu and sigma and evaluates the log likelihood. The function to compute the prior log likelihood is as follows:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"function prior_loglike(μ, σ)\n    LL = 0.0\n    LL += logpdf(Normal(0, 1), μ)\n    LL += logpdf(truncated(Cauchy(0, 1), 0, Inf), σ)\n    return LL\nend","category":"page"},{"location":"gaussian/#Define-Sample-Prior-Distribution","page":"Gaussian Model","title":"Define Sample Prior Distribution","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Each particle in DifferentialEvolution must be seeded with an initial value. To do so, we define a function that returns the initial value. A common approach is to use the prior distribution as it is intended to encode likely values of mu and sigma. The function for sampling from the prior distribution of mu and sigma is given in the following code block:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"function sample_prior()\n    μ = rand(Normal(0, 1))\n    σ = rand(truncated(Cauchy(0, 1), 0, Inf))\n    return [μ,σ]\nend","category":"page"},{"location":"gaussian/#Define-Log-Likelihood-Function","page":"Gaussian Model","title":"Define Log Likelihood Function","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Next, we define a function to compute the log likelihood of the data. The first argument must be the data followed by a separate argument for each parameter, maintaining the same order specified in prior_loglike. In our case, we can write the log likelihood function with data followed by mu and sigma:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"function loglike(data, μ, σ)\n    return sum(logpdf.(Normal(μ, σ), data))\nend","category":"page"},{"location":"gaussian/#Define-Bounds-and-Parameter-Names","page":"Gaussian Model","title":"Define Bounds and Parameter Names","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"We must define the lower and upper bounds of each parameter. The bounds are a Tuple of tuples, where the i^mathrmth inner tuple contains the lower and upper bounds of the i^mathrmth parameter. Note that the parameters must be in the same order as specified in prior_loglike and loglike. In the present case, we only have mu followed by sigma:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"bounds = ((-Inf,Inf),(0.0,Inf))","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"We can also pass a Tuple of names for each parameter, again in the same order as specified in prior_loglike and loglike:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"names = (:μ,:σ)","category":"page"},{"location":"gaussian/#Define-Model-Object","page":"Gaussian Model","title":"Define Model Object","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Now that we have defined the necessary components of our model, we will organize them into a DEModel object as follows:","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"model = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names)","category":"page"},{"location":"gaussian/#Define-Sampler","page":"Gaussian Model","title":"Define Sampler","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"Next, we will create a sampler with the constructor DE. Here, we will pass the sample_prior function and the variable bounds, which constains the lower and upper bounds of each parameter. In addition, we will specify 1000 burnin samples, Np=6 particles per group (default 4 particles per group).","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"de = DE(;sample_prior, bounds, burnin=1000, Np=6)","category":"page"},{"location":"gaussian/#Estimate-Parameter","page":"Gaussian Model","title":"Estimate Parameter","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"The code block below runs the sampler for 2000 trials with each group of particles running on a separate thread. The progress bar is also set to display. ","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"n_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"gaussian/#Convergence","page":"Gaussian Model","title":"Convergence","text":"","category":"section"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"We will evaluate convergence using two methods. First, we will verify that hatr leq 105 in the chain summary above. Indeed, it is. Its also important to examine the trace plot for any evidence of non-stationarity, or getting stuck at the same value for long periods of time. In the left panel of the plot below, the samples for each chain display the characteristic behavior indicative of efficient sampling and convergence: they look like white noise, or a \"hairy caterpillar\". The plot on the right shows the posterior distribution of mu and sigma for each of the 12 chains.","category":"page"},{"location":"gaussian/","page":"Gaussian Model","title":"Gaussian Model","text":"plot(chains, grid=false)","category":"page"},{"location":"#DifferentialEvolutionMCMC.jl","page":"home","title":"DifferentialEvolutionMCMC.jl","text":"","category":"section"},{"location":"","page":"home","title":"home","text":"Documentation for DifferentialEvolutionMCMC is under construction.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(88484)\n\ndata = (N = 10,k = 6)\n\nprior_loglike(θ) = logpdf(Beta(1, 1), θ)\n\nsample_prior() = rand(Beta(1, 1))\n\nbounds = ((0,1),)\nnames = (:θ,)\n\nfunction loglike(data, θ)\n    (;N,k) = data\n    return logpdf(Binomial(N, θ), k)\nend\n\nmodel = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names\n)\n\nde = DE(;sample_prior, bounds, burnin=1000, Np=3, σ=.01)\nn_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"binomial/#Binomial-Example","page":"Binomial Model","title":"Binomial Example","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"This simple example using a binomial model will guide you through the process of performing Bayesin parameter estimation with Differential Evolution MCMC. Suppose we observe N samples from a random binomial process and observe k successes. Formally, this can be stated as:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"k sim mathrmBinomial(N theta)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"where theta is a parameter representing the probability of success. Our goal is to estimate the probability of success theta from the k. Let's assume the prior distribution of theta is given by","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"theta sim mathrmbeta(1 1)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"In this simple case, the posterior distribution of theta has a simple closed-form solution:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"theta_k sim mathrmbeta(1 + k 1 + N - k)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":".","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We will use this fact to verify that the MCMC sampler is working correctly. ","category":"page"},{"location":"binomial/#Load-Packages","page":"Binomial Model","title":"Load Packages","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Our first step is to load the required packages including StatsPlots.jl for plotting the MCMC chain.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"using DifferentialEvolutionMCMC\nusing Random\nusing Distributions\nusing StatsPlots\nRandom.seed!(88484)","category":"page"},{"location":"binomial/#Define-Data","page":"Binomial Model","title":"Define Data","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Let's assume we make N=10 observations which produce k=6 successes. ","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"data = (N = 10,k = 6)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The true posterior distribution is given by:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"true_posterior = Beta(1 + 6, 1 + 4)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The maximum likelihood estimate of theta is","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"hattheta_mathrmMLE = frackN = 060","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The mean of the posterior of theta will be slightly less because our prior distribution encodes one failure and one success:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"mean(true_posterior)","category":"page"},{"location":"binomial/#Define-Prior-Log-Likelihood-Function","page":"Binomial Model","title":"Define Prior Log Likelihood Function","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The next step is to define a function that passes our parameter theta and evaluates the log likelihood. The function to compute the prior log likelihood is as follows:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"prior_loglike(θ) = logpdf(Beta(1, 1), θ)","category":"page"},{"location":"binomial/#Define-Sample-Prior-Distribution","page":"Binomial Model","title":"Define Sample Prior Distribution","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Each particle in DifferentialEvolution must be seeded with an initial value. To do so, we define a function that returns the initial value. A common approach is to use the prior distribution as it is intended to encode likely values of theta. The function for sampling from the prior distribution of theta is given in the following code block:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"sample_prior() = rand(Beta(1, 1))","category":"page"},{"location":"binomial/#Define-Log-Likelihood-Function","page":"Binomial Model","title":"Define Log Likelihood Function","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Next, we define a function to compute the log likelihood of the data. The first argument must be the data followed by a separate argument for each parameter, maintaining the same order specified in prior_loglike. In our case, we can write the log likelihood function as follows:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"function loglike(data, θ)\n    (;N,k) = data\n    return logpdf(Binomial(N, θ), k)\nend","category":"page"},{"location":"binomial/#Define-Bounds-and-Parameter-Names","page":"Binomial Model","title":"Define Bounds and Parameter Names","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We must define the lower and upper bounds of each parameter. The bounds are a Tuple of tuples, where the i^mathrmth inner tuple contains the lower and upper bounds of the i^mathrmth parameter. Note that the parameters must be in the same order as specified in prior_loglike and loglike. In the present case, we only have a single parameter theta:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"bounds = ((0,1),)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We can also pass a Tuple of names for each parameter, again in the same order as specified in prior_loglike and loglike:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"names = (:θ,)","category":"page"},{"location":"binomial/#Define-Model-Object","page":"Binomial Model","title":"Define Model Object","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Now that we have defined the necessary components of our model, we will organize them into a DEModel object as follows:","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"model = DEModel(; \n    sample_prior, \n    prior_loglike, \n    loglike, \n    data,\n    names)","category":"page"},{"location":"binomial/#Define-Sampler","page":"Binomial Model","title":"Define Sampler","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Next, we will create a sampler with the constructor DE. Here, we will pass the sample_prior function and the variable bounds, which constains the lower and upper bounds of each parameter. In addition, we will specify 1000 burnin samples, Np=3 particles per group (default 4 particles per group) and proposal noise of σ=.01.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"de = DE(;sample_prior, bounds, burnin=1000, Np=3, σ=.01)","category":"page"},{"location":"binomial/#Estimate-Parameter","page":"Binomial Model","title":"Estimate Parameter","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"The code block below runs the sampler for 2000 trials with each group of particles running on a separate thread. The progress bar is also set to display. ","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"n_iter = 2000\nchains = sample(model, de, MCMCThreads(), n_iter, progress=true)","category":"page"},{"location":"binomial/#Evaluation","page":"Binomial Model","title":"Evaluation","text":"","category":"section"},{"location":"binomial/#Convergence","page":"Binomial Model","title":"Convergence","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"We will evaluate convergence using two methods. First, we will verify that hatr leq 105 in the chain summary above. Indeed, it is. Its also important to examine the trace plot for any evidence of non-stationarity, or getting stuck at the same value for long periods of time. In the left panel of the plot below, the samples for each chain display the characteristic behavior indicative of efficient sampling and convergence: they look like white noise, or a \"hairy caterpillar\". The plot on the right shows the posterior distribution of theta for each of the 12 chains.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"plot(chains, grid=false)","category":"page"},{"location":"binomial/#Accuracy","page":"Binomial Model","title":"Accuracy","text":"","category":"section"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"Let's also see if the results are similar to the closed-form solution. Below, we see that the mean and standard deviation are similar to the chain summary in the Estimate Parameter section, suggesting that the MCMC sampler is working as expected.","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"mean(true_posterior)","category":"page"},{"location":"binomial/","page":"Binomial Model","title":"Binomial Model","text":"std(true_posterior)","category":"page"}]
}
